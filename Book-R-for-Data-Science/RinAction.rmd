---
title: "R in Action Exercise"
author: "Chen Qingqing"
date: "March 26, 2018"
output: 
  rmarkdown::html_document:
    toc: true # table of content true 
    toc_depth: 3 # upto three depths of headings (spcified by #, ##, and ###)
    number_sections: true ## if you want number sections at each table header
    theme: spacelab # many options for theme, this one is my facorite.
    highlight: tango # specifies the syntax highlighting style
    toc_float: true # you can add your custom css. should be in same folder
    keep_md: true 
    #code_folding: hide
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      warning = FALSE,
                      message = FALSE,
                      eval = TRUE,
                      fig.path = "Figures/",
                      #fig.width = 3, 
                      #fig.height = 3,
                      results="markup", 
                      collapse = TRUE,
                      tidy = TRUE
                      #tidy.opts=list(width.cutoff=30)
                      )
```

# Introduction of R 
## Getting help 
- `help.start()`:help open tutorial website 
- `help("ddd")`: "ddd" help 
- `help.search("...")` or `?...`: 
- `example("foo")`
- `RSiteSearch("foo")`
- 'apropos("foo", mode="function")`: list all functions that contains "foo"
- `data()`: list all data 
- `vignette()`: list all vignette docs 

## Workspace 
Functions for manageing workspace 

- `getwd()`: get currently workspace directory
- `setwd("")`: set workspace directory 
- `ls()`: list objects in current workspace
- `rm(objectlist)`: delete one or multiple objects 
- `help(objects)`: show the explanatory of selected objects 
- `options()`: show or set current selections
- `history(#)`: show certain number of history command, default is 25
- `savehistory("myfile)`: save history to myfile, default as `.Rhistory`
- `loadhistory("myfile")`: load myfile history
- `save.image("myfile")`: save workspace as myfile, default as `.RData`
- `save(objectlist, file="myfile)`: save selected bojects to a file named myfile
- `load("myfile")`: read workspace
- `q()`: quite R 

## Input and output
- `source("myfile")`: input 
- `sink("myfile", append=TRUE, split=TRUE)`: output, _append_ and _split_ set to TRUE, the output will not cover the file but add after the file 
- Output for image 
  - `pdf("filename.pdf")`: pdf file
  - `win.metafile("filename.wmf")`: windows image file
  - `png("filename.png")`: png file
  - `jpeg("filename.jpeg")`: JPEG file 
  - `bmp("filename.bmp")`: BMP file
  - `postscript("filename.ps")`: PostScript file 

## packages 
- `.libPaths()`: library path 
- `install.package("package_name")`: install package 
- `library(package_name)`: load package
- `help(package="package_name")`: package help 

# Creating a Dataset 
- **Structures for holding data**: scalars, vectors, arrays, data frames, and lists. 
- **Data types or modes**:  numeric, character, logical (TRUE/FALSE), complex (imaginary numbers), and raw (bytes). 
## Data Structure
### Vectors 
- **Vectors** are **one-dimensional arrays** that hold numerica data, charactoer data, or logical data
- `c()` is used to form the vector
- Vector must <span style="color:red"> only be one type or mode (numeric, charactor, or logical)</span>
- **Scalars** are **one-element** vectors
- Using **a numeric vector of positions within brackets** to refer elements of a vector 

```{r vector}
a <- c(1,2,5,3,6,-2,4)
a[3]
```

### Matrices 
- **Matrix** is a _two-dimensional array_ where each element has the <span style="color:red">same mode (numeric, character, or logical)</span>

- `mymatrix <- matrix(vector, nrow = number_of_rows, ncol = number_of_columns,`
`byrow = logical_value, dimnames = list(char_vector_rownames, char_vector_colnames))`
- Identify rows, columns, or elements of a matrix by using subscripts and brackets. X[i,] refers to the ith row of matrix, X[,j] refers to jth column, X[i,j] refers to the ijth element

```{r matrix}
b <- matrix(1:10, nrow = 2, ncol = 5, byrow = FALSE, dimnames = list(c("a","b"), c("c","d","e","f","g")))
b
b[2,]
b[,2]
b[2,4]
b[1,c(4,5)]
```

### Arrays
- **Arrays** are similar to matrices but can have more than two dimensions
- `myarray<-array(vector, dimnsions, dimnames)`
  - vectro: contains the data for the array
  - dimensions: a numeric vector giving the maximal index for each dimension 
  - dimnames: an optional list of dimension label 

```{r array}
dim1 <- c("A1","A2")
dim2 <- c("B1","B2","B3")
dim3 <- c("C1","C2","C3","C4")
z <- array(1:24, c(2,3,4), dimnames = list(dim1,dim2, dim3))
z
```

### Data frames
- Different columns contain different modes of data (numeric, character, etc)
- `mydata <- data.frame(col1, col2, col3,...)`

```{r creating_data_frame}
patientID <- c(1,2,3,4)
age <- c(25,34,28,52)
diabetes <- c("Type1","Type2","Type1","Type1")
status <- c("Poor","Improved", "Excellent", "Poor")
patientdata <- data.frame(patientID, age, diabetes, status, row.names = patientID) # case identifiers, patientID is used to identify individuals in the dataset 
patientdata
```

```{r specifyling_elements_of_a_data_frame}
patientdata[1:2]
patientdata[c("diabetes","status")]
patientdata$age
```

- `attach()`: The `attach()` function adds the data frame to the R search path.
- `detach()`: The `detach()` function removes the data frame from the search path
  - The `attach()` and `detach()` functions are best used when you're analyzing a single data frame and you're unlikely to have multiple objects with the same name. 
  
```{r attache_detache}
summary(mtcars$mpg)
plot(mtcars$mpg, mtcars$disp)
# this is the same as below 
attach(mtcars)
        summary(mpg)
        plot(mpg, disp)
detach(mtcars)
```
  
- `with()`:
  - the `with()` function is that assignments will **only exist within** the function brackets.
  - If you need to create objects that will exist outside of the `with()` construct, use the special assignment operator `<<-` instead of the standard one (`<-`)
  
```{r with}
with(mtcars, {
        summary(mpg, disp, wt)
        plot(mpg, disp)
}) #If there's only one statement (for example, summary(mpg)), the {} brackets are optional.

with(mtcars, {
  nokeepstats <<- summary(mpg)  
  keepstats <<- summary(mpg)
})
nokeepstats
```

### Factors 
- Variables can be described as _nominal, ordinal, or continuous_
  - **Nominal variables**:  are categorical, without an implied order
  - **Ordinal variables**:  imply order but not amount
  - **Continuous variables**: can take on any value within some range, and both order and amount are implied
- **Categorical (nominal)** and **ordered categorical (ordinal)** variables in R are called **factors** - The function `factor()` stores the categorical values as a vector of integers in the range [1... k] (where k is the number of unique values in the nominal variable), and an internal vector of character strings (the original values) mapped to these integers 

```{r factor}
diabetes <- c("Type1","Type2","Type1","Type1")
diabetes
str(diabetes)
diabetes <- factor(diabetes)
diabetes
str(diabetes)
```

```{r factor2}
patientID <- c(1, 2, 3, 4)
age <- c(25, 34, 28, 52)
diabetes <- c("Type1", "Type2", "Type1", "Type1")
status <- c("Poor", "Improved", "Excellent", "Poor")
diabetes <- factor(diabetes) 
status <- factor(status, order=TRUE, levels = c("Poor","Improved","Excellent"))
patientdata <- data.frame(patientID, age, diabetes, status)
str(patientdata)
summary(patientdata)
```

### List 
- A list is an ordered collection of objects (components). 
- A list allows you to gather a variety of (possibly unrelated) objects under one name
- `mylist <- list(object1, object2, ...)`
- can name the objects in a list:
  - `mylist <- list(name1=object1, name2=object2, ...)`
- specify elements of the list by indicating a component number or a name **within double brackets**.

```{r list}
g <- "My First List"
h <- c(25, 26, 18, 39)
j <- matrix(1:10, nrow=5)
k <- c("one", "two", "three")
mylist <- list(title=g, ages=h, j, k)
mylist
mylist[[2]]
mylist[["ages"]]
```

## Data Input 
### Entering data from the keyboard
- `edit()`: invoke a text editor that will allow you to enter your data manually
- If you don't assign it a destination, all of your edits will be **lost**!
- A shortcut for `mydata <- edit(mydata)` is simply `fix(mydata)`.
`mydata <- data.frame(age=numeric(0),`
  `gender=character(0), weight=numeric(0))`
`mydata <- edit(mydata)`


### Importing data from a delimited text file
- `mydataframe <- read.table(file, header=logical_value, sep="delimiter", row.names="name")`
  - _file_: a delimited ASCII file
  - _header_: a logical value indicating whether the first row contains variable names (**TRUE or FALSE**)
  - _sep_: specifies the delimiter separating data valuee
  - _row.names_: an optional parameter specifying one or more variables to represent row identifiers
- By default, character variables are converted to factors. This behavior may not always be desirable (for example, a variable containing respondents' comments). You can suppress this behavior in a number of ways. Including the option `stringsAsFactors=FALSE` will turn this behavior off for all character variables. Alternatively, you can use the `colClasses` option to specify a class (for example, logical, numeric, character, factor) for each column.
- `file()`: allows the user to access files, the clipboard, and C-level standard input. 
- `gzfile(), bzfile(), xzfile()`, and `unz()` functions let the user read compressed files
- `url()`:access internet files through a complete URL that includes `http://, ftp://, or file://`

### Importing data from Excel
- The best way to read an Excel file is to export it to a comma-delimited file from within Excel and import it to R using the method described earlier. 
  - `RODBC` package
  - `library(RODBC)`
     `channel <- odbcConnectExcel("myfile.xls")`
     `mydataframe <- sqlFetch(channel, "mysheet")`
     `odbcClose(channel)`
- Excel 2007 uses an XLSX file format, which is essentially a zipped set of XML files. The xlsx package can be used to access spreadsheets in this format.
  - `xlsx` package 
  - `library(xlsx)`
        `workbook <- "c:/myworkbook.xlsx"`
        `mydataframe <- read.xlsx(workbook, 1)`

### Importing data from XML
- **XML package** written by Duncan Temple Lang allows users to read, write, and manipulate XML files

### Webscraping
- One way to accomplish this is to download the web page using the `readLines()` function and manipulate it with functions such as `grep()` and `gsub()`. 
- For complex web pages, the `RCurl` and `XML` packages can be used to extract the information desired.
- [Webscraping using readLines and RCurl](http://www.programmingr.com/content/webscraping-using-readlines-and-rcurl/)

`library("RCurl") #install.packages("RCurl", dependencies = TRUE) `
`library("XML") #install.packages("XML", dependencies = TRUE)`
`jan09 <- getURL("https://stat.ethz.ch/pipermail/r-help/2009-January/date.html", ssl.verifypeer = FALSE) # Get first quarter archives `
`jan09_parsed <- htmlTreeParse(jan09)`
`author_lines <- jan09_parsed[grep("<I>", jan09_parsed)] # Pull out the appropriate line`
`authors <- gsub("<I>", "", author_lines, fixed = TRUE) # Delete unwanted characters in the lines we pulled out`
`author_counts <- sort(table(authors), decreasing = TRUE) # Present only the ten most frequent posters
`author_counts[1:10]`


### Importing data from SPSS
- SPSS datasets can be imported into R via the `read.spss()` function in the `foreign` package
- use the `spss.get()` function in the `Hmisc` package. `spss.get()` is a wrapper function that automatically sets many parameters of read.
- `library(Hmisc)`
        `mydataframe <- spss.get("mydata.sav", use.value.labels=TRUE)`

### Importing data from SAS
- `read. ssd()` in the `foreign` package and `sas.get()` in the `Hmisc` package.
- `mydata <- read.table("mydata.csv", header=TRUE, sep=",")`

### Importing data from Stata
- `library(foreign) mydataframe <- read.dta("mydata.dta")`

### Importing data from netCDF
- Unidata's netCDF (network Common Data Form) open source software contains ma- chine-independent data formats for the creation and distribution of array-oriented sci- entific data
- The `ncdf` package provides support for data files created with Unidata???s netCDF library (version 3 or earlier) and is available for Windows, Mac OS X, and Linux platforms.
- `library(ncdf) nc <- nc_open("mynetCDFfile") myarray <- get.var.ncdf(nc, myvar)`

### Importing data from HDF5
- HDF5 (Hierarchical Data Format) is a software technology suite for the management of extremely large and complex data collections
- The `hdf5` package can be used to write R objects into a file in a form that can be read by software that understands the HDF5 format

### Accessing database management systems (DBMSs)
- R can interface with a wide variety of **relational database management systems (DBMSs)**, including **Microsoft SQL Server, Microsoft Access, MySQL, Oracle, PostgreSQL, DB2, Sybase, Teradata, and SQLite**
  - The **ODBC** Interface: `install.packages("RODBC")`
  ![](/Users/qingqing/Data_Science/R_in_Action/Figures/shot1.png)

    - `library(RODBC)`
`myconn <-odbcConnect("mydsn", uid="Rob", pwd="aardvark")`
`crimedat <- sqlFetch(myconn, Crime)`
`pundat <- sqlQuery(myconn, "select * from Punishment")`
`close(myconn)`
  - **DBI-RELATED** Packages
    - The DBI package provides a general and consistent client-side interface to DBMS
    - Check the documentation on CRAN (http://cran.r-project.org) for details.


### Importing data via Stat/Transfer
- **Stat/Transfer** (www.stattransfer.com) is a stand-alone application that can transfer data between 34 data formats, in- cluding R

## Annotating datasets
Data analysts typically annotate datasets to make the results easier to interpret.

### Variable labels
- `names(patientdata)[2] <- "admissionAge"`

### Value labels
- The `factor()` function can be used to create value labels for categorical variables
- `patientdata$gender <- factor(patientdata$gender,`
                                     `levels = c(1,2),`
`labels = c("male", "female"))`

## Useful functions for working with data objects
![](/Users/qingqing/Data_Science/R_in_Action/Figures/shot2.png)

# Getting Start With Graphs
## Working with graphs
- Save the graph as a PDF document named mygraph.pdf in the current directory 
- In addition to `pdf()`, you can use the functions `win.metafile()`, `png()`, `jpeg()`, `bmp()`, `tiff()`, `xfig()`, and `postscript()` to save graphs in other formats.

```{r graph1}
pdf("mygraph.pdf")
    attach(mtcars)
        plot(wt, mpg)
        abline(lm(mpg~wt))
        title("Regression of MPG on Weight")
    detach(mtcars)
dev.off()
```
## Graphical parameters
- Customize many features of a graph (fonts, colors, axes, titles) through options called _graphical parameters_.
- `par(optionname=value, optionname=value, ...)` function: Values set in this manner will be in effect for the rest of the session or until they???re changed
- Adding the `no.readonly=TRUE` option produces a list of current graphical settings that **can be modified**.

```{r graph2}
dose  <- c(20, 30, 40, 45, 60)
drugA <- c(16, 20, 27, 40, 60)
drugB <- c(15, 18, 25, 31, 40)

opar <- par(no.readonly = TRUE)
    par(mfrow=c(1,2)) # 1 row 2 column 
    
    par(lty = 2, pch = 17)
    plot(dose, drugA, type = "b")
    
    # can combine in the same line for high quality function like plot()
    plot(dose, drugA, type = "b", lty = 2, pch = 17) 
par(opar)
```

## Symbols and lines
- Parameters for specifying symbols and lines
![](/Users/qingqing/Data_Science/R_in_Action/Figures/shot3.png)

![](/Users/qingqing/Data_Science/R_in_Action/Figures/shot4.png)

## Colors
- Parameters for specifying color
![](/Users/qingqing/Data_Science/R_in_Action/Figures/shot5.png)

- Can specify colors in R by **index, name, hexadecimal, RGB, or HSV**. For example, `col=1, col="white", col="#FFFFFF", col=rgb(1,1,1), and col=hsv(0,0,1)` are equivalent ways of specifying the **color white**
  - The function `rgb()` creates colors based on **red-green-blue values**, whereas `hsv()` creates colors based on **hue-saturation values**
- R also has a number of functions that can be used to create **vectors of contiguous colors**. These include `rainbow(), heat.colors(), terrain. colors(), topo.colors(), and cm.colors()`

```{r graph3}
n <- 10
mycolors <- rainbow(n)
mygrays <- gray(0:n/n)

opar <- par(no.readonly = TRUE)
    par(mfrow=c(1,2))
    pie(rep(1, n), labels=mycolors, col=mycolors)
    pie(rep(1, n), labels=mygrays, col=mygrays)
par(opar)
```

## Text characteristics
- Parameters specifying text size
![](/Users/qingqing/Data_Science/R_in_Action/Figures/shot6.png)

- Parameters specifying font family, size, and style
![](/Users/qingqing/Data_Science/R_in_Action/Figures/shot7.png)

- If graphs will be output in **PDF** or **PostScript** format, changing the font family is relatively straightforward. 
  - **PDFs**: use `names(pdfFonts())` to find out which fonts are available on your system and `pdf(file="myplot.pdf", family="fontname")` to generate the plots. 
  - **PostScript**: use `names(postscriptFonts())` and `postscript(file="myplot.ps", family="fontname")`
  
## Graph and margin dimensions
Control the plot dimensions and margin sizes using the parameters
- Parameters for graph and margin dimensions
![](/Users/qingqing/Data_Science/R_in_Action/Figures/shot8.png)
  
  - e.g: `par(pin=c(4,3), mai=c(1,.5, 1, .2))`  
 
```{r graph4}
dose  <- c(20, 30, 40, 45, 60)
drugA <- c(16, 20, 27, 40, 60)
drugB <- c(15, 18, 25, 31, 40)
opar <- par(no.readonly = TRUE)
par(pin=c(2,2.5))
par(mfrow=c(1,2))
    plot(dose, drugA, type = "b", pch=19, lty = 2, col = "red", lwd=3, cex=1.5, cex.axis = 0.75, font.axis=1)
    plot(dose, drugA, type = "b", pch=23, lty = 6, col = "blue", bg="green",lwd=1, cex=1, cex.axis = 0.5, font.axis=4, main="Clinical Trials for Drug A", cex.main = 1, font.main = 2, sub="This is hypothetical data", cex.sub = 0.5, font.sub = 4, xlab="Dosage", ylab="Drug Response", cex.lab = 0.5, font.sub = 3, xlim=c(0, 60), ylim=c(0, 70))
par(opar)

```
 

## Adding text, customized axes, and legends
### Titles
- ```title(main="main title", sub="sub-title", xlab="x-axis label", ylab="y-axis label")```
- `title(main="My Title", col.main="red",`
  `sub="My Sub-title", col.sub="blue",`
  `xlab="My X label", ylab="My Y label",`
  `col.lab="green", cex.lab=0.75)`  
  
### Axes
- `axis(side, at=, labels=, pos=, lty=, col=, las=, tck=, ...)`
![](/Users/qingqing/Data_Science/R_in_Action/Figures/shot9.png)

- options `xaxt="n""` and `yaxt="n"" suppress the x- and y-axis, respectively (leaving the frame lines, without ticks)
- Some high-level plotting functions include default titles and labels. You can remove them by adding `ann=FALSE` in the `plot()` statement or in a separate `par()` statement.

```{r graph5}
# specify data
x <- c(1:10)
y <- x
z <- 10/x

opar <- par(no.readonly=TRUE)
# increase margin
par(mar=c(5, 4, 4, 8) + 0.1)
# plot x versus y 
plot(x, y, type="b", pch=21, col="red", yaxt="n", lty=3, ann=FALSE) # suppress y-axis leaving the frame lines, without ticks, ann=FALSE remove default titles and labels

# Add x versus z line
lines(x, z, type="b", pch=22, col="blue", lty=2)

# draw axes 
axis(2, at=x, labels=x, col.axis="red", las=2)
axis(4, at=z, labels=round(z, digits=2),
     col.axis="blue", las=2, cex.axis=0.7, tck=-.01)

# add title and text 
mtext("y=1/x", side=4, line=3, cex.lab=0.75, las=2, col="blue")
title("An Example of Creative Axes",
      xlab="X values",
      ylab="Y=X", col.lab = "red")
par(opar)
```
  
- Minor Tick Marks:  `library(Hmisc) minor.tick(nx=n, ny=n, tick.ratio=n)`
  - `nx` and `ny` specify the number of intervals in which to divide the area between major tick marks on the x-axis and y-axis, respectively. 
  - `tick.ratio` is the size of the minor tick mark relative to the major tick mark
  
### Reference lines
- `abline(h=yvalues, v=xvalues)`
- `abline(v=seq(1, 10, 2), lty=2, col="blue")`, adds dashed blue vertical lines at x = 1, 3, 5, 7, and 9
  
### Legend
- `legend(location, title, legend, ...)`
![](/Users/qingqing/Data_Science/R_in_Action/Figures/shot10.png)
![](/Users/qingqing/Data_Science/R_in_Action/Figures/shot10.1.png)  


```{r graph6}
dose  <- c(20, 30, 40, 45, 60)
drugA <- c(16, 20, 27, 40, 60)
drugB <- c(15, 18, 25, 31, 40)

opar <- par(no.readonly = TRUE)
plot(dose, drugA, type = "b", pch=15, lty=1, col="red", lwd=1.5, cex=1, font.lab=2, ylim = c(0,60), main="Drug A vs. Drug B", col.main = "green",xlab="Drug Dosage", ylab="Drug Response", col.lab = "green", cex.main = 1, cex.lab=1)
lines(dose, drugB, type="b", pch=17, lty=2, col="blue", lwd = 0.75, cex = 1, font.lab = 3)
abline(h=c(30), lwd=1.5, lty=2, col="gray")
# Add minor tick marks
library(Hmisc)
minor.tick(nx=3, ny=3, tick.ratio=0.5)
# Add legend
legend("topleft", inset=.05, title="Drug Type", c("A","B"),
       lty=c(1, 2), pch=c(15, 17), col=c("red", "blue"))
par(opar)

```
  
### Text annotations  
- `text(location, "text to place", pos, ...)`
- `mtext("text to place", side, line=n, ...)` 
![](/Users/qingqing/Data_Science/R_in_Action/Figures/shot11.png) 

```{r graph7}
attach(mtcars)
plot(wt, mpg,
     main="Mileage vs. Car Weight",
     xlab="Weight", ylab="Mileage",
     pch=18, col="blue")
text(wt, mpg,
     row.names(mtcars),
     cex=0.6, pos=4, col="red")
detach(mtcars)
```

```{r graph8}
opar <- par(no.readonly=TRUE)
par(cex.lab=0.75)
plot(1:7,1:7,type="n", cex=1.5)
text(3,3,"Example of default text")
text(4,4,family="mono","Example of mono-spaced text")
text(5,5,family="serif","Example of serif text")
par(opar)
```

- `demo(plotmath)`
![](/Users/qingqing/Data_Science/R_in_Action/Figures/shot12.png) 

## Combining graphs
- `par()`: can include the graphical parameter `mfrow=c(nrows, ncols)` to create a matrix of nrows x ncols plots that are **filled in by row**, `mfcol=c(nrows, ncols)` to **fill the matrix by columns**

```{r comine1}
attach(mtcars)
    opar <- par(no.readonly=TRUE)
        par(mfrow=c(2,2))
        plot(wt,mpg, main="Scatterplot of wt vs. mpg")
        plot(wt,disp, main="Scatterplot of wt vs disp")
        hist(wt, main="Histogram of wt")
        boxplot(wt, main="Boxplot of wt")
    par(opar)
detach(mtcars)
```

- `layout()`: has the form `layout(mat)` where mat is a matrix object specifying the **location of the multiple plots to combine**
```{r combine2}
attach(mtcars)
    layout(matrix(c(1,2,3,3), 2, 2, byrow = TRUE), widths=c(2, 1), heights=c(2, 1))
    hist(wt)
    hist(mpg)
    hist(disp)
detach(mtcars)
```

### Creating a figure arrangement with fine control
```{r fine_control}
opar <- par(no.readonly=TRUE)
par(fig=c(0, 0.8, 0, 0.8)) #c(x1,x2,y1,y2)
plot(mtcars$wt, mtcars$mpg, xlab="Miles Per Gallon", ylab="Car Weight") #Set up scatter plot
par(fig=c(0, 0.8, 0.55, 1), new=TRUE) # new=TRUE adding a figure to an existing graph
boxplot(mtcars$wt, horizontal=TRUE, axes=FALSE) # Add box plot above
par(fig=c(0.65, 1, 0, 0.8), new=TRUE)
boxplot(mtcars$mpg, axes=FALSE) # Add box plot to right
mtext("Enhanced Scatterplot", side=3, outer=TRUE, line=-2)
par(opar)

```


# Basic Data Management 
## Creating new variables
- `variable <- expression`
- Arithmetic operators
![](/Users/qingqing/Data_Science/R_in_Action/Figures/shot13.png)

```{r creating_new_variables}
# method1
mydata <- data.frame(x1 = c(2,2,6,4),
                     x2 = c(3,4,2,8))
mydata$sumx <- mydata$x1 + mydata$x2
mydata$meanx <- (mydata$x1 + mydata$x2)/2

# method2
attach(mydata)
    mydata$sumx <- x1 + x2
    mydata$meanx <- (x1+x2)/2
detach(mydata)
    
#method3
mydata <- transform(mydata, 
                    sumx = x1 + x2,
                    meanx = (x1+x2)/2)
```

## Recoding variables
- Change a continuous variable into a set of categories
- Replace miscoded values with correct values
- Create a pass/fail variable based on a set of cutoff scores
- Logical operators
  - `within()` function is similar to the `with()` function, but allows you to modify the data frame
![](/Users/qingqing/Data_Science/R_in_Action/Figures/shot14.png)
- Several packages offer useful recoding functions; 
  - The `car` package's `recode()` function: recodes numeric and character vectors and factors very simply. 
  - The package `doBy` offers `recodevar()`: another popular function. 
  - R ships with `cut()`: allows you to divide the range of a numeric variable into intervals, returning a factor.

## Renaming variables
- Mannually: `fix(name_of_dataframe)`
- Programmatically: 
  - `rename(dataframe, c(oldname="newname", oldname="newname",...))`
  - `names()`: e.g: `names(leadership)[2] <- "testDate"`

## Missing values
- In R, missing values are represented by:
  - the symbol **NA (not available)**???`is.na()`, missing values are considered noncomparable, even to themselves. This means that you can???t use comparison operators to test for the presence of missing values.
  - the symbol NaN (not a number): Impossible values (for example, dividing by 0) 
- Unlike programs such as SAS, R uses the same missing values symbol for character and numeric data.

### Recoding values to missing
- e.g: `leadership$age[leadership$age == 99] <- NA`

### Excluding missing values from analyses
- `na.rm=TRUE`

## Date values
- `as.Date(x, "input_format")`
- Data formats
![](/Users/qingqing/Data_Science/R_in_Action/Figures/shot15.png)
![](/Users/qingqing/Data_Science/R_in_Action/Figures/shot16.png)

- `Sys.Date()` returns today???s date
- `date()` returns the current date and time
- `format()` takes an argument and applies an output format : `today <- Sys.Date(), format(today, format="%A")`
- When R stores dates internally, they???re represented as the number of days since **January 1, 1970**, with negative values for earlier dates. That means you **can perform arithmetic operations on them**
- `difftime()`: calculate a time interval and express it as seconds, minutes, hours, days, or weeks

```{r date}
startdate <- as.Date("2004-02-13")
enddate   <- as.Date("2011-01-22")
enddate - startdate

today <- Sys.Date()
dob   <- as.Date("1991-12-07")
difftime(today, dob, units="weeks")
```

### Converting dates to character variables
- `strDates <- as.character(dates)`

### Going further
- learn more about converting character data to dates, take a look at:
  - `help(as. Date)` 
  - `help(strftime)`
- learn more about formatting dates and times
  - `help(ISOdatetime)`
- The `lubridate` package contains a number of functions that simplify working with dates, including functions to identify and parse date-time data, extract date-time components (for example, years, months, days, etc.), and perform arithmetic calculations on date-times.
- do complex calculations with dates, the `fCalendar` package can also help. It provides a myriad of functions for deal- ing with dates, can handle multiple time zones at once, and provides sophisticated calendar manipulations that recognize business days, weekends, and holidays.

## Type conversions
- Type conversion functions
![](/Users/qingqing/Data_Science/R_in_Action/Figures/shot17.png)

## Sorting data
- `order()` function: e.g: `newdata <- leadership[order(leadership$age),]`

## Merging datasets
### Adding columns
- `total <- cbind(A, B)`
- `total <- merge(dataframeA, dataframeB, by="ID")`
- `total <- merge(dataframeA, dataframeB, by=c("ID","Country"))`

### Adding rows
- `total <- rbind(dataframeA, dataframeB)`
- If dataframeA has variables that dataframeB doesn???t, then before joining them do one of the following:
  - Delete the extra variables in dataframeA
  - Create the additional variables in dataframeB and set them to NA (missing)

## Subsetting datasets
### Selecting (keeping) variables
- `dataframe[row indices, column indices]`: can add using `paste()`


### Excluding (dropping) variables
- e.g: `myvars <- names(leadership) %in% c("q3", "q4") newdata <- leadership[!myvars]`
- e.g: `leadership$q3 <- leadership$q4 <- NULL`: Note that NULL isn???t the same as NA (missing).

### Selecting observations
- e.g: `newdata <- leadership[which(leadership$gender=="M" & leadership$age > 30),]`:  `which()` gives the indices of a vector that are TRUE

### The subset() function
- `subset()` function is probably the easiest way to select variables and observation: e.g: `newdata <- subset(leadership, age >= 35 | age < 24, select=c(q1, q2, q3, q4))`

### Random samples
- `mysample <- leadership[sample(1:nrow(leadership), 3, replace=FALSE),]`

## Using SQL statements to manipulate data frames
- `install.packages("sqldf")`
- can use the `sqldf()` function to apply `SQL SELECT` statements to data frames

```{r sql}
library(sqldf)
newdf <- sqldf("select * from mtcars where carb=1 order by mpg",
                          row.names=TRUE)
newdf
```

# Advanced Data Management 
## Numerical and character functions
### Mathematical functions
![](/Users/qingqing/Data_Science/R_in_Action/Figures/shot18.png)
![](/Users/qingqing/Data_Science/R_in_Action/Figures/shot19.png)

### Statistical functions
![](/Users/qingqing/Data_Science/R_in_Action/Figures/shot20.png)
![](/Users/qingqing/Data_Science/R_in_Action/Figures/shot21.png)

- Standarizing Data
  - `scale()` function standardizes the specified columns of a matrix or data frame to a **mean of 0 and a standard deviation of 1**: `newdata <- scale(mydata)`
  - To standardize each column to an **arbitrary mean and standard deviation**, you can use code similar to the following:`newdata <- transform(mydata, myvar = scale(myvar)*10+50)` (standardizes the variable myvar to a mean of 50 and standard deviation of 10)
  
- Probability functions
  - `[dpqr]distribution_abbreviation()`, where the first letter refers to the aspect of the distribution returned:
    - d = density
    - p = distribution function
    - q = quantile function
    - r = random generation (random deviates)
  - Probability distributions
![](/Users/qingqing/Data_Science/R_in_Action/Figures/shot22.png)
  - Normal distribution functions (continued)
  ![](/Users/qingqing/Data_Science/R_in_Action/Figures/shot23.png)
  - SETTING THE SEED FOR RANDOM NUMBER GENERATION:  
    - `set.seed()`: make your results reproducible
    - `runif()` function: generate pseudo-random numbers from a uniform distribution on the interval 0 to 1
```{r seeds}
runif(5)
runif(5)
set.seed(1234)
runif(5)
set.seed(1234)
runif(5)
```
  - GENERATING MULTIVARIATE NORMAL DATA: `mvrnorm(n, mean, sigma)` :`MASS package`
```{r multivariate_normal_data}
library(MASS)
options(digits=3)
set.seed(1234) #Set random number seed
mean <- c(230.7, 146.7, 3.6)
sigma <- matrix(c(15360.8, 6721.2, -47.1,6721.2, 4700.9, -16.5, -47.1,  -16.5,   0.3), nrow=3, ncol=3) #Specify mean vector, covariance matrix
mydata <- mvrnorm(500, mean, sigma)
mydata <- as.data.frame(mydata)
names(mydata) <- c("y","x1","x2")
head(mydata, n=3)
```
  
### Character functions
- Note that the functions grep(), sub(), and strsplit() can search for a text string (fixed=TRUE) or a regular expression (fixed=FALSE) (FALSE is the default)
![](/Users/qingqing/Data_Science/R_in_Action/Figures/shot24.png)

### Other useful functions
- Use `\n` for new lines,`\t` for tabs, `\'` for a single quote, `\b` for backspace, and so forth (type `?Quotes` for more information)
![](/Users/qingqing/Data_Science/R_in_Action/Figures/shot25.png)

### Applying functions to matrices and data frames
- `apply(x, MARGIN, FUN, ...)` 
  - **x**: data object
  - **MARGIN**: dimension index, MARGIN=1 indicates rows and MARGIN=2 indicates columns
  - **FUN**: a function you specify
  - **...**: any parameters you want to pass to FUN

```{r apply}
mydata <- matrix(rnorm(30), nrow=6)
mydata
apply(mydata, 1, mean) # Calculate row means
apply(mydata, 2, mean) # Calculate column means
apply(mydata, 2, mean, trim=0.2) #calculate trimmed column means (in this case, means based on the middle 60 percent of the data, with the bottom 20 percent and top 20 percent of values discarded)
```

## A solution for our data management challenge
```{r solution}
options(digits=2)
Student <- c("John Davis", "Angela Williams", "Bullwinkle Moose","David Jones", "Janice Markhammer", "Cheryl Cushing","Reuven Ytzrhak", "Greg Knox", "Joel England","Mary Rayburn")
Math <- c(502, 600, 412, 358, 495, 512, 410, 625, 573, 522)
Science <- c(95, 99, 80, 82, 75, 85, 80, 95, 89, 86)
English <- c(25, 22, 18, 15, 20, 28, 15, 30, 27, 18)
roster <- data.frame(Student, Math, Science, English,stringsAsFactors=FALSE)
# normalization
z <- scale(roster[,2:4]) 
# Obtain performance scores
score <- apply(z, 1, mean)  
roster <- cbind(roster, score)
# Grade students
y <- quantile(score, c(.8,.6,.4,.2)) 
roster$grade[score >= y[1]] <- "A"
roster$grade[score < y[1] & score >= y[2]] <- "B"
roster$grade[score < y[2] & score >= y[3]] <- "C"
roster$grade[score < y[3] & score >= y[4]] <- "D"
roster$grade[score < y[4]] <- "F"
# Extract last and first names
name <- strsplit((roster$Student), " ")
lastname <- sapply(name, "[", 2) #"[" is a function that extracts part of an object
firstname <- sapply(name, "[", 1)
roster <- cbind(firstname,lastname, roster[,-1]) #-1 means delete first column "Student"
#Sort by last and first names
roster <- roster[order(lastname,firstname),]
roster
```

## Control flow
For the syntax examples throughout this section, keep the following in mind:
  - **statement** is a single R statement or a compound statement (a group of R state- ments enclosed in curly braces { } and separated by semicolons).
  - **cond** is an expression that resolves to true or false.
  - **expr** is a statement that evaluates to a number or character string.
  - **seq** is a sequence of numbers or character strings.

### Repetition and looping
- **FOR**: The for loop executes a statement repetitively until a variable???s value is no longer con- tained in the sequence seq. The syntax is `for (var in seq) statement`
- **WHILE**: A while loop executes a statement repetitively until the condition is no longer true. The syntax is `while (cond) statement`

### Conditional execution
- **IF-ELSE**: The if-else control structure executes a statement if a given condition is true. Optionally, a different statement is executed if the condition is false. The syntax is:`if (cond) statement` `if (cond) statement1 else statement2`
- **IFELSE**: The ifelse construct is a compact and vectorized version of the if-else construct. The syntax is: `ifelse(cond, statement1, statement2)`
- **SWITCH**: switch chooses statements based on the value of an expression. The syntax is `switch(expr, ...)`

```{r switch}
feelings <- c("sad", "afraid")
for (i in feelings)
    print(
     switch(i,
            happy  = "I am glad you are happy",
            afraid = "There is nothing to fear",
            sad    = "Cheer up",
            angry  = "Calm down now") 
         )
```

### User-written functions
`myfunction <- function(arg1, arg2, ... ){ statements return(object) }`

## Aggregation and restructuring
### Transpose
- `t()` function to transpose a matrix or a data frame
### Aggregating data
- `aggregate(x, by, FUN)`: where x is the data object to be collapsed, by is a list of variables that will be crossed to form the new observations, and FUN is the scalar function used to calculate summary statistics that will make up the new observation values.

```{r aggregating_data}
options(digits=3)
attach(mtcars)
aggdata <-aggregate(mtcars, by=list(cyl,gear), FUN=mean, na.rm=TRUE)
aggdata
detach(mtcars)
```

### The reshape package
- **melting**: e.g: `md <- melt(mydata, id=(c("id", "time")))`
- **casting**: e.g: `newdata <- cast(md, formula, FUN)`

# Basic Graphs
The data are contained in the `Arthritis` data frame distributed with the `vcd` package.
## Bar plots
### Simple bar plots
```{r bar1}
library(vcd)
counts <- table(Arthritis$Improved)
par(mfrow=c(2,2))
barplot(counts,
        main="Simple Bar Plot",
        xlab="Improvement", ylab="Frequency")
barplot(counts,
        main="Horizontal Bar Plot",
        xlab="Frequency", ylab="Improvement",
        horiz=TRUE)
# If the categorical variable to be plotted is a factor or ordered factor, you can create a vertical bar plot quickly with the plot() function
plot(Arthritis$Improved, main="Simple Bar Plot", xlab="Improved", ylab="Frequency")
plot(Arthritis$Improved, horiz=TRUE, main="Horizontal Bar Plot", xlab="Frequency", ylab="Improved")

```

### Stacked and grouped bar plots
```{r stacked_bar_plots}
counts <- table(Arthritis$Improved, Arthritis$Treatment)
counts
par(mfrow=c(1,2))
par(cex = 0.75)
barplot(counts,
        main="Stacked Bar Plot",
        xlab="Treatment", ylab="Frequency",
        col=c("red", "yellow","green"),
        legend=rownames(counts))  # beside=FALSE (the default), then each column of the matrix produces a bar in the plot, with the values in the column giving the heights of stacked sub-bars
barplot(counts,
        main="Grouped Bar Plot",
        xlab="Treatment", ylab="Frequency",
        col=c("red", "yellow", "green"),
        legend=rownames(counts), beside=TRUE) #beside=TRUE, each column of the matrix represents a group, and the values in each column are juxtaposed rather than stacked
```

### Mean bar plots
Bar plots needn???t be based on counts or frequencies. You can create bar plots that represent means, medians, standard deviations, and so forth by using the `aggregate` function and passing the results to the `barplot()` function.
```{r mean_bar_plot}
states <- data.frame(state.region, state.x77)
means <- aggregate(states$Illiteracy, by=list(state.region), FUN=mean)
means <- means[order(means$x),]
means
barplot(means$x, names.arg=means$Group.1)
title("Mean Illiteracy Rate")
```

### Tweaking bar plots
`names.arg` argument allows you to specify a character vector of names used to label the bars
```{r tweaking_bar_plots}
par(mfrow=c(1,2))
par(mar=c(5,8,4,2))
par(las=0) #rotated the bar labels 
counts <- table(Arthritis$Improved)
barplot(counts,
           main="Treatment Outcome",
           horiz=TRUE, cex.names=0.8,
        names.arg=c("No Improvement", "Some Improvement",
                    "Marked Improvement"))
par(mar=c(5,8,4,2))
par(las=2) #rotated the bar labels 
counts <- table(Arthritis$Improved)
barplot(counts,
           main="Treatment Outcome",
           horiz=TRUE, cex.names=0.8,
        names.arg=c("No Improvement", "Some Improvement",
                    "Marked Improvement"))
```

### Spinograms
Spinogram: a stacked bar plot is rescaled so that the height of each bar is 1 and the segment heights represent _proportions_. Spinograms are created through the `spine()` function of the `vcd` package

```{r spinograms}
library(vcd)
attach(Arthritis)
counts <- table(Treatment, Improved)
spine(counts, main="Spinogram Example")
detach(Arthritis)
```

## Pie charts
- `pie(x, labels)`
```{r pie_charts}
par(mfrow=c(2, 2))
slices <- c(10, 12, 4, 16, 8)
lbls <- c("US", "UK", "Australia", "Germany", "France")

pie(slices, labels = lbls, main="Simple Pie Chart")

pct <- round(slices/sum(slices)*100)
lbls2 <- paste(lbls, " ", pct, "%", sep="")
pie(slices, labels=lbls2, col=rainbow(length(lbls2)), main="Pie Chart with Percentages")


library(plotrix)
pie3D(slices, labels=lbls,explode=0.1, main="3D Pie Chart ") #pie3D() function from the plotrix package

mytable <- table(state.region)
lbls3 <- paste(names(mytable), "\n", mytable, sep="")
pie(mytable, labels = lbls3, main="Pie Chart from a Table\n (with sample sizes)")
```

- `fan.plot()` function in the `plotrix` package.
```{r fan_plot}
library(plotrix)
slices <- c(10, 12,4, 16, 8)
lbls <- c("US", "UK", "Australia", "Germany", "France")
fan.plot(slices, labels = lbls, main="Fan Plot")
```

## Histograms
- display the distribution of a **continuous** variable by dividing up the range of scores into a specified number of bins on the x-axis and displaying the frequency of scores in each bin on the y-axis: `hist(x)`
  - The option `freq=FALSE` creates a plot based on probability densities rather than frequencies. 
  - The `breaks` option controls the number of bins. 
  - The default produces equally spaced breaks when defining the cells of the histogram

```{r histograms1}
par(mar=c(5,8,4,2))
par(mfrow=c(2,2))
# Simple histogram
hist(mtcars$mpg) 
# With specified w bins and color
hist(mtcars$mpg,
     breaks=12,
     col="red",
     xlab="Miles Per Gallon",
     main="Colored histogram with 12 bins")

#With rug plot and frame
hist(mtcars$mpg,
     freq=FALSE,
     breaks=12,
     col="red",
     xlab="Miles Per Gallon",
     main="Histogram, rug plot, density curve")
rug(jitter(mtcars$mpg))
lines(density(mtcars$mpg), col="blue", lwd=2)

#With normal curve
x <- mtcars$mpg
h <- hist(x,
        breaks=12,
        col="red",
        xlab="Miles Per Gallon",
        main="Histogram with normal curve and box")
xfit<-seq(min(x), max(x), length=40)
yfit<-dnorm(xfit, mean=mean(x), sd=sd(x))
yfit <- yfit*diff(h$mids[1:2])*length(x)
lines(xfit, yfit, col="blue", lwd=2)
box()
```

## Kernel density plots
- `plot(density(x))`
```{r kernel_density_plots}
par(mfrow=c(2,1))
d <- density(mtcars$mpg)
plot(d)

d <- density(mtcars$mpg)
plot(d, main="Kernel Density of Miles Per Gallon")
polygon(d, col="red", border="blue")
rug(mtcars$mpg, col="brown")
```

### Comparative kernel density plots
```{r comparative_kernel}
par(lwd=2)
library(sm)
attach(mtcars)
#Create grouping factor
cyl.f <- factor(cyl, levels= c(4,6,8),
              labels = c("4 cylinder", "6 cylinder",
                         "8 cylinder"))
#Plot densities
sm.density.compare(mpg, cyl, xlab="Miles Per Gallon")
title(main="MPG Distribution by Car Cylinders")

#Add legend via mouse click
colfill<-c(2:(1+length(levels(cyl.f))))
legend("topright",locator(1), levels(cyl.f),fill=colfill) #locator(1) place the legend interactively
detach(mtcars)
```

## Box plots
![](/Users/qingqing/Data_Science/R_in_Action/Figures/shot26.png)

### Using parallel box plots to compare groups
- `boxplot(formula, data=dataframe)`
  - a formula is `y ~ A`, where a separate box plot for numeric variable y is generated for each value of categorical variable A. 
  - a formula `y ~ A*B` would produce a box plot of numeric variable y, for each combination of levels in categorical variables A and B.
```{r boxplots}
par(mfrow=c(1,2))
boxplot(mpg ~ cyl, data=mtcars,
        main="Car Mileage Data",
        xlab="Number of Cylinders",
        ylab="Miles Per Gallon")

boxplot(mpg ~ cyl, data=mtcars,
        notch=TRUE, #get notched box plots, two boxes??? notches don???t overlap, there???s strong evidence that their medians differ
        varwidth=TRUE, #produces box plots with widths that are proportional to their sample sizes
        col="red",
        main="Car Mileage Data",
        xlab="Number of Cylinders",
        ylab="Miles Per Gallon")
```
  
### Box plots for two crossed factors
```{r boxplots2}
# Create factor for number of cylinders
mtcars$cyl.f <- factor(mtcars$cyl,
                       levels=c(4,6,8),
                       labels=c("4","6","8"))
#Create factor for transmission type
mtcars$am.f <- factor(mtcars$am,
                      levels=c(0,1),
                      labels=c("auto", "standard"))
#Generate box plot
boxplot(mpg ~ am.f *cyl.f,
        data=mtcars,
        varwidth=TRUE,
        col=c("gold","darkgreen"),
        main="MPG Distribution by Auto Type",
        xlab="Auto Type")
```

### Violin plots
- `vioplot(x1, x2, ... , names=, col=)`
```{r violins}
library(vioplot)
x1 <- mtcars$mpg[mtcars$cyl==4]
x2 <- mtcars$mpg[mtcars$cyl==6]
x3 <- mtcars$mpg[mtcars$cyl==8]
vioplot(x1, x2, x3,
        names=c("4 cyl", "6 cyl", "8 cyl"),
        col="gold")
        title("Violin Plots of Miles Per Gallon")
```

## Dot plots
- Dot plots provide a method of plotting a large number of labeled values on a simple horizontal scale
  - `dotchart(x, labels=)`

```{r dotplots}
par(mfrow=c(1,2))
dotchart(mtcars$mpg, labels=row.names(mtcars), cex=.7,
         main="Gas Mileage for Car Models",
         xlab="Miles Per Gallon")
x <- mtcars[order(mtcars$mpg),]
x$cyl <- factor(x$cyl)
x$color[x$cyl==4] <- "red"
x$color[x$cyl==6] <- "blue"
x$color[x$cyl==8] <- "darkgreen"
dotchart(x$mpg,
         labels = row.names(x),
         cex=.7,
         groups = x$cyl,
         gcolor = "black",
         color = x$color,
         pch=19,
         main = "Gas Mileage for Car Models\ngrouped by cylinder",
         xlab = "Miles Per Gallon")
```

# Basic Statistic
## Descriptive statistics
### A menagerie of methods
- `summary()`: provides the **minimum, maximum, quartiles, and the mean** for numerical variables and **frequencies** for factors and logical vectors
- `sapply()`: `sapply(x, FUN, options)`, typical functions that you can plug in here are **mean, sd, var, min, max, median, length, range, and quantile**
- `fivenum()`: returns Tukey???s five-number summary (**minimum, lower-hinge, median, upper-hinge, and maximum**)
- Several user-contributed packages offer functions for descriptive statistics, including `Hmisc`, `pastecs`, and `psych`
  - `describe()` function in the `Hmisc` package returns **the number of variables and observations, the number of missing and unique values, the mean, quantiles, and the five highest and lowest values**
  - `pastecs` package includes a function named `stat.desc()` that provides a wide range of descriptive statistics. `stat.desc(x, basic=TRUE, desc=TRUE, norm=FALSE, p=0.95)`
    - x: data frame or time series. 
    - If `basic=TRUE (the default)`, the number of values, null values, missing values, minimum, maximum, range, and sum are provided. 
    - If `desc=TRUE (also the default)`, the median, mean, standard error of the mean, 95 percent confidence interval for the mean, variance, standard deviation, and coefficient of variation are also provided. 
    - If `norm=TRUE (not the default)`, normal distribution statistics are returned, including skewness and kurtosis (and their statistical significance), and the Shapiro???Wilk test of normality. 
    - A `p-value` option is used to calculate the confidence interval for the mean (.95 by default)
  - the `psych` package also has a function called `describe()` that provides the number of nonmissing observations, mean, standard deviation, median, trimmed mean, median absolute deviation, minimum, maximum, range, skew, kurto- sis, and standard error of the mean
    - If you want the `Hmisc` version instead, you can type `Hmisc::describe(mt)`
    
```{r descriptive_statistics_via_sapply}
vars <- c("mpg","hp","wt")
mystats <- function(x, na.omit=FALSE){
    if(na.omit)
        x <- x[!is.na(x)]
    m = mean(x)
    n = length(x)
    s = sd(x)
    skew <- sum((x-m)^3/s^3)/n
    kurt <- sum((x-m)^4/s^4)/n - 3
    return(c(n = n, mean = m, stdev = s, skew = skew, kurtosis = kurt))
}

sapply(mtcars[vars], mystats)
```

```{r descriptive_Hmisc_describe}
library(Hmisc)
describe(mtcars[vars])
```

```{r descriptive_pastecs_describe}
library(pastecs)
stat.desc(mtcars[vars], basic = TRUE, desc = TRUE, norm = TRUE, p = 0.95)
```

```{r descriptive_psych_describe}
library(psych)
Hmisc::describe(mtcars[vars]) # can use Hmisc::describe(mtcars[vars]) if you want the describe from Hmisc package
```
  
### Descriptive statistics by group
- **using aggregate()**: only allows to use single value functions such as mean, standard deviation
   -  If you had used `list(mtcars$am)`, the `am` column would have been labeled `Group.1` rather than `am`. You use the assignment to provide a more useful column label. 
  - If you have more than one grouping variable, you can use code like `by=list(name1=groupvar1, name2=groupvar2, ... , groupvarN)`
- **by() function**: 
  - data is a data frame or matrix
  - INDICES is a factor or list of factors that define the groups
  - FUN is an arbitrary function
- **doBy package**: `summaryBy(formula, data=dataframe, FUN=function)`
  - where the formula takes the form `var1 + var2 + var3 + ... + varN ~ groupvar1 + groupvar2 + ... + groupvarN`
  - Variables on the left of the `~` are the numeric variables to be analyzed and variables on the right are categorical grouping variables
- **psych package** : `describe.by()`
- **reshape package**: 
  - First, you `melt` the data frame using `dfm <- melt(dataframe, measure.vars=y, id.vars=g)`: where dataframe contains the data, y is a vector indicating the numeric variables to be summarized (the default is to use all), and g is a vector of one or more grouping variables
  - Then `cast` the data using `cast(dfm, groupvar1 + groupvar2 + ... + variable ~ ., FUN)`: where the grouping variables are separated by + signs, the word variable is entered exactly as is, and FUN is an arbitrary function.
  
```{r group_aggregate}
#Descriptive statistics by group using aggregate()
aggregate(mtcars[vars], by=list(am=mtcars$am), mean)
```
  
```{r descriptive_groupby_doby}
library(doBy)
summaryBy(mpg+hp+wt~am, data=mtcars, FUN=mystats)
```

```{r descriptive_groupby_describe_by}
library(psych)
describe.by(mtcars[vars], mtcars$am)
```

```{r descriptive_groupby_reshape}
library(reshape)
dstats <- function(x)(c(n=length(x), mean=mean(x), sd=sd(x)))
dfm <- melt(mtcars, measure.vars=c("mpg","hp","wt"), id.vars = c("am","cyl"))
cast(dfm, am + cyl + variable ~ ., dstats)
```

## Frequency and contingency tables
### Generating frequency tables
![](/Users/qingqing/Data_Science/R_in_Action/Figures/shot27.png)

- One-way tables 
```{r table_frequency}
library(vcd)
mytable <- with(Arthritis, table(Improved)) #generate simple frequency counts
mytable
prop.table(mytable) #frequencies into proportions
prop.table(mytable)*100 # into percentages
```

- Two-way tables: 
  - `mytable <- table(A, B)`: where A is the row variable, and B is the column variable
  - `mytable <- xtabs(~ A + B, data=mydata)`: `xtabs()` function allows you to create a contingency table using formula style input
    - the variables to be cross-classified appear on the right of the formula (that is, to the right of the ~) separated by + signs. If a variable is included on the left side of the formula, it???s assumed to be a vector of frequencies (useful if the data have already been tabulated)
    - `margin.table()`: marginal frequencies 
    - `prop.table()`: proportions 
    - `addmargins()`: add marginal sums to table
    - The `table()` function ignores missing values (NAs) by default. To include NA as a valid category in the frequency counts, include the table option `useNA="ifany"`.
  - `CrossTable()`: in the `gmodels` package


```{r two_way_tables}
mytable <- with(Arthritis, table(Treatment, Improved ))
mytable
mytable_2 <- xtabs(~Treatment+Improved, data = Arthritis)
mytable_2
```

```{r margin_table}
margin.table(mytable,1) # index (1) refers to the first variable in the table() statement
margin.table(mytable,2) # index (2) refers to the second variable in the table() statement
prop.table(mytable,1)
prop.table(mytable,2)
prop.table(mytable)  #Cell proportions
```

```{r addmargin_sum_table}
addmargins(mytable)
addmargins(prop.table(mytable)) #the default is to create sum margins for all variables in a table
prop.table(mytable,1)
addmargins(prop.table(mytable,1),2) #adds a sum column alone
prop.table(mytable,2)
addmargins(prop.table(mytable,2),1) #adds a sum row alone
```

```{r gmodels_table}
library(gmodels)
CrossTable(Arthritis$Treatment, Arthritis$Improved)
```

- Multidimensional Tables
  - `table()` and `xtabs()`:  generate multidimensional tables based on three or more categorical variables. 
  - `margin.table()`, `prop.table()`, and `addmargins()` functions:  extend naturally to more than two dimensions. 
  - `ftable()` function: used to print multidimensional tables in a compact and attractive manner.

```{r multidimensional_table}
mytable <- xtabs(~ Treatment+Sex+Improved, data=Arthritis) # Cell frequencies
mytable
ftable(mytable)
margin.table(mytable, 1) # Marginal frequencies
margin.table(mytable, 2)
margin.table(mytable, 3)
margin.table(mytable, c(1, 3)) # Treatment Improved marginal frequencies
ftable(prop.table(mytable, c(1, 2))) # Improve proportions for  Treatment x Sex
ftable(addmargins(prop.table(mytable, c(1, 2)), 3))
ftable(addmargins(prop.table(mytable, c(1,2)), 3)) * 100
```

### Tests of independence
- **Chi-Square Test of Independence**: apply the function `chisq.test()` to a two-way table in order to produce a chi-square test of independence of the row and column variables.
  - The p-values are the probability of ob- taining the sampled results assuming independence of the row and column variables in the population.
  
```{r chi_square_test}
library(vcd)
mytable <- xtabs(~Treatment+Improved, data = Arthritis)
chisq.test(mytable)
mytable <- xtabs(~Improved+Sex, data=Arthritis)
chisq.test(mytable)
```
  
- **Fisher Exact Test**: evaluates the null hypothesis of independence of rows and columns in a contingency table with fixed marginals. 
```{r fisher_exact_test}
mytable <- xtabs(~Treatment+Improved, data = Arthritis)
fisher.test(mytable)
```
  
- **Cochran-Mantel-Haenszel Test**: The `mantelhaen.test()` function provides a Cochran???Mantel???Haenszel chi-square test of the null hypothesis that two nominal variables are conditionally independent in each stratum of a third variable.

```{r cochran_mantel_haenszel_test}
mytable <- xtabs(~Treatment+Improved+Sex, data=Arthritis)
mantelhaen.test(mytable)
```

### Measures of association
- The significance tests in the previous section evaluated whether or not sufficient evidence existed to reject a null hypothesis of independence between variables.
- If you can reject the null hypothesis, your interest turns naturally to measures of association in order to gauge the strength of the relationships present. 
- The `assocstats()` function in the `vcd package` can be used to calculate the `phi coefficient`, `contingency coefficient`, and `Cramer???s V` for a two-way table.
```{r association}
library(vcd)
mytable <- xtabs(~Treatment+Improved, Arthritis)
assocstats(mytable)
```

### Converting tables to flat files
- Converting a table into a flat file via table2flat:
```{r table2flat}
table2flat <- function(mytable){
    df <- as.data.frame(mytable)
    rows <- dim(df)[1]
    cols <- dim(df)[2]
    x <- NULL
    for (i in 1:rows){
        for (j in 1:df$Freq[i]){
            row <- df[i,c(1:(cols-1))]
            x <- rbind(x,row)
        }
    }
    row.names(x) <- c(1:dim(x)[1])
    return(x)
}
```

```{r table2flat2}
treatment <- rep(c("Placebo", "Treated"), times=3)
improved <- rep(c("None", "Some", "Marked"), each=2)
Freq <- c(29,13,7,17,7,21)
mytable <- as.data.frame(cbind(treatment, improved, Freq))
mydata <- table2flat(mytable)
head(mydata)
```
  
## Correlations
### Types of correlations
- **Pearson, Spearman & Kendall correlations**: 
  - The **Pearson product moment correlation** assesses the degree of linear relationship between two quantitative variables. 
  - **Spearman???s Rank Order correlation coefficient** assesses the degree of relationship between two rank-ordered variables. 
  - **Kendall???s Tau** is also a nonparametric measure of rank correlation.
  - The `cor()` function produces all three correlation coefficients, whereas the `cov()` function provides covariances.     - There are many options, but a simplified format for producing correlations is `cor(x, use= , method= )`
    - x: Matrix or data frame.
    - use: Specifies the handling of missing data. The options are all.obs (assumes no missing data???missing data will produce an error), everything (any correlation involving a case with missing values will be set to missing), complete.obs (listwise deletion), and pairwise.complete.obs (pair wise deletion).
    - method: Specifies the type of correlation. The options are pearson, spearman, or kendall.
  
```{r correlations}
states<- state.x77[,1:6]
cov(states)
cor(states)
cor(states, method="spearman")
```
  
```{r correlations2}
x <- states[,c("Population", "Income", "Illiteracy", "HS Grad")]
y <- states[,c("Life Exp", "Murder")]
cor(x,y)
```
  
- **Partial Corelations**: 
  - is a correlation between two quantitative variables, controlling for one or more other quantitative variables. 
  - can use the `pcor()` function in the `ggm` package to provide partial correlation coefficients. 
  - `pcor(u, S)`: where u is a vector of numbers, with the first two numbers the indices of the variables to be correlated, and the remaining numbers the indices of the conditioning variables; S is the covariance matrix among the variables. 
  
```{r partial_corelations}
library(ggm)
# partial correlation of population and murder rate, controlling
# for income, illiteracy rate, and HS graduation rate
pcor(c(1,5,2,3,6), cov(states))
```
  
### Testing correlations for significance
- `cor.test(x, y, alternative = , method = )`
  - `x` and `y` are the variables to be correlated, alternative specifies a two-tailed or one- tailed test ("two.side", "less", or "greater") 
  - `method` specifies the type of correlation (`"pearson", "kendall", or "spearman"`) to compute. 
  - Use `alternative="less"` when the research hypothesis is that the population correlation is less than 0. 
  - Use `alternative="greater"` when the research hypothesis is that the population correlation is greater than 0. By default, `alternative="two.side"` (population correlation isn???t equal to 0) is assumed.
```{r testing_correlations_for_significance}
cor.test(states[,3], states[,5])
```
  
- Unfortunately, you can test only one correlation at a time using `cor.test`. 
- Luckily, the `corr.test()` function provided in the `psych` package allows you to go further. The `corr.test()` function produces correlations and significance levels for matrices of `Pearson, Spearman, or Kendall correlations`.
  - The `use=` options can be `"pairwise"` or `"complete"` (for pairwise or listwise deletion of missing values, respectively). 
  - The `method=` option is `"pearson"` (the default), `"spearman"`, or `"kendall"`. 
  
```{r  via_corr_test}
library(psych)
corr.test(states, use = "complete")
```

## t-tests
### Independent t-test
- `t.test(y~x, data)`: y is numeric and x is a dichotomous variable, or `t.test(y1, y2)`, y1 and y2 are numeric vectors (the outcome variable for each group)
  - can add a `var.equal=TRUE` option to specify equal variances and a pooled variance estimate. 
  - By default, a two-tailed alternative is assumed (that is, the means differ but the direction isn???t specified). 
  - can add the option `alternative="less"` or `alternative="greater"` to specify a directional test.
```{r t_test}
library(MASS)
t.test(Prob ~ So, data = UScrime)
```

### Dependent t-test
- `t.test(y1, y2, paired=TRUE)`, y1 and y2 are the numeric vectors for the two dependent groups.

```{r dependent_t_test}
library(MASS)
sapply(UScrime[c("U1","U2")], function(x)(c(mean=mean(x),sd=sd(x))))
with(UScrime, t.test(U1, U2, paired=TRUE))
```

## Nonparametric tests of group differences
### Comparing two groups
- If the two groups are _independent_, you can use the **Wilcoxon rank sum test** (more popularly known as the **Mann???Whitney U test**) to assess whether the observations are sampled from the same probability distribution (that is, whether the probability of obtaining higher scores is greater in one population than the other). The format is either
`wilcox.test(y ~ x, data)` where y is numeric and x is a dichotomous variable, or `wilcox.test(y1, y2)` where y1 and y2 are the outcome variables for each group.

```{r mann-whitney}
with(UScrime, by(Prob, So, median))
wilcox.test(Prob ~ So, data = UScrime)
```

### Comparing more than two groups
- The format for the **Kruskal???Wallis** test is `kruskal.test(y ~ A, data)`, where y is a numeric outcome variable and A is a grouping variable with two or more levels (if there are two levels, it???s equivalent to the Mann???Whitney U test). 
- For the **Friedman test**, the format is `friedman.test(y ~ A | B, data)`, where y is the numeric outcome variable, A is a grouping variable, and B is a blocking variable that identifies matched observations. 
- In both cases, data is an option argument specifying a matrix or data frame containing the variables.
  
# Regression 
## The many faces of regression
- Varieties of regression analysis
![](/Users/qingqing/Data_Science/R_in_Action/Figures/28.png)

### Scenarios for using OLS regression
In OLS regression, a quantitative dependent variable is predicted from a weighted sum of predictor variables, where the weights are parameters estimated from the data.

## OLS regression
![](/Users/qingqing/Data_Science/R_in_Action/Figures/29.png)

### Fitting regression models with lm()
- `myfit <- lm(formula, data)`
  - `formula` describes the model to be fit, typically written as `Y ~ X1 + X2 + ... + Xk`, where the `~` separates the response variable on the left from the predictor variables on the right, and the predictor variables are separated by `+` signs. 
  - `data` is the data frame containing the data to be used in fitting the model. 
  - The resulting object (myfit in this case) is a list that contains extensive information about the fitted model. 
![](/Users/qingqing/Data_Science/R_in_Action/Figures/30.png)

![](/Users/qingqing/Data_Science/R_in_Action/Figures/31.png)  
  
### Simple linear regression
```{r Simple_linear_regression}
fit <- lm(weight ~ height, data = women)
summary(fit)
women$weight
fitted(fit)
residuals(fit)
plot(women$height, women$weight,
     xlab = "Height (in inches)",
     ylab = "Weight (in pounds)")
abline(fit)
```

### Polynomial regression
- You might be able to improve your prediction using a regression with a quadratic term (that is, X^2^).
```{r polynomial_regression}
fit2 <- lm(weight ~ height + I(height^2), data=women)
summary(fit2)
plot(women$height,women$weight,
            xlab="Height (in inches)",
            ylab="Weight (in lbs)")
lines(women$height,fitted(fit2), col = "red")
```

```{r scatterplot}
library(car)
scatterplot(weight ~ height, 
            data = women,
            spread = FALSE, 
            lty.smooth = 2,
            pch = 19, 
            main = "Women Age 30-39", 
            xlab = "Height (in inches)",
            ylab = "Weight (lbs.)")
```

### Multiple linear regression
```{r multiple_linear_regression}
states <- as.data.frame(state.x77[,c("Murder", "Population", "Illiteracy","Income", "Frost")])
#Examining bivariate relationships
cor(states)
library(car)
scatterplotMatrix(states, spread = FALSE, lty.smooth=2, main = "Scatter Plot Matrix")
fit <- lm(Murder ~ Population + Illiteracy + Income + Frost, data = states)
summary(fit)
```

### Multiple linear regression with interactions
- A significant interaction between two predictor variables tells you that the relationship between one predictor and the response variable depends on the level of the other predictor. 

```{r multiple_linear}
fit <- lm(mpg ~ hp+wt+hp:wt, data = mtcars)
summary(fit)
```

- Can visualize interactions using the `effect()` function in the `effects` package. The format is
`plot(effect(term, mod, xlevels), multiline=TRUE)`
  - `term` is the quoted model term to plot, 
  - `mod` is the fitted model returned by lm(), 
  - `xlevels` is a list specifying the variables to be set to constant values and the values to employ. 
  - `multiline=TRUE` option superimposes the lines being plotted.
  
```{r effect}
fit <- lm(mpg ~ hp+wt+hp:wt, data = mtcars)
library(effects)
plot(effect("hp:wt", fit,
   xlevels = list(wt=c(2.2,3.2,4.2))),
   multiline=TRUE)
```

## Regression diagnostics
```{r confint_function}
fit <- lm(Murder ~ Population + Illiteracy + Income + Frost, data = states)
confint(fit)

```
- The results suggest that you can be 95 percent confident that the interval [2.38, 5.90] contains the true change in murder rate for a 1 percent change in illiteracy rate. Additionally, because the confidence interval for Frost contains 0, you can conclude that a change in temperature is unrelated to murder rate, holding the other variables constant. But your faith in these results is only as strong as the evidence that you have that your data satisfies the statistical assumptions underlying the model.

- A set of techniques called **regression diagnostics** provides you with the necessary tools for evaluating the appropriateness of the regression modeland can help you to uncover and correct problems. First, we???ll start with a standard approach that uses functions that come with R???s base installation. Then we???ll look at newer, improved
methods available through the car package.

### A typical approach
- The most common approach is to apply the `plot()` function to the object returned by the lm(). Doing so produces four graphs that are useful for evaluating the model fit. 
```{r typical_approach}
fit <- lm(weight ~ height, data = women)
par(mfrow = c(2,2))
plot(fit)
```

- To understand these graphs, consider the assumptions of OLS regression:
  - **Normality**: If the dependent variable is normally distributed for a fixed set of predictor values, then the residual values should be normally distributed with a mean of 0. The Normal Q-Q plot (upper right) is a probability plot of the standardized residuals against the values that would be expected under normality. If you???ve met the normality assumption, the points on this graph should fall on the **straight 45-degree line**. Because they don???t, you???ve clearly violated the normality assumption.
  - **Independence**: You can???t tell if the dependent variable values are independent from these plots. You have to use your understanding of how the data were col- lected. 
  - **Linearity**: If the dependent variable is linearly related to the independent vari- ables, there should be no systematic relationship between the residuals and the predicted (that is, fitted) values. In other words, the model should capture all the systematic variance present in the data, leaving nothing but random noise. In the Residuals versus Fitted graph (upper left), you see clear evidence of a curved relationship, which suggests that you may want to add a quadratic term to the regression.
  - **Homoscedasticity**: If you???ve met the constant variance assumption, the points in the Scale-Location graph (bottom left) should be a random band around a horizontal line. You seem to meet this assumption.

- The graph identifies **outliers, high-leverage points, and influential observations**. Specifically:
  - An **outlier** is an observation that isn???t predicted well by the fitted regression model (that is, has a large positive or negative residual).
  - An observation with a **high leverage value** has an unusual combination of predictor values. That is, it???s an outlier in the predictor space. The dependent variable value isn???t used to calculate an observation???s leverage.
  - An **influential observation** is an observation that has a disproportionate impact on the determination of the model parameters. Influential observations are identified using a statistic called Cook???s distance, or Cook???s D.

```{r diagnostic_plots_for_the_quadratic_fit}
fit2 <- lm(weight ~ height + I(height^2), data = women)
par(mfrow=c(2,2))
confint(fit2)
plot(fit2)
```

```{r diagnostic_plots_for_the_quadratic_fit2}
newfit <- lm(weight ~ height + I(height^2), data = women[-c(13,15), ])
par(mfrow=c(2,2))
plot(newfit)
```

### An enhanced approach
![](/Users/qingqing/Data_Science/R_in_Action/Figures/32.png)
- In addition, the `gvlma` package provides a global test for linear model assumptions.
- Let???s look at each in turn, by applying them to our multiple regression example.
  - NORMALITY: The `qqPlot()` function provides a more accurate method of assessing the normality assumption than provided by the `plot()` function in the base package. It plots the **studentized residuals** (also called **studentized deleted residuals or jackknifed residuals**) against a t distribution with **n-p-1** degrees of freedom, where **n** is the sample size and **p** is the number of regression parameters (including the intercept). 
  
```{r normality}
library(car)
fit <- lm(Murder ~ Population + Illiteracy + Income + Frost, data=states)
qqPlot(fit, labels=row.names(states), id.method="identify",
  simulate=TRUE, main="Q-Q Plot")
states["Nevada",]
fitted(fit)["Nevada"]
residuals(fit)["Nevada"]
rstudent(fit)["Nevada"]
```
    - Another way of visualizing errors
      - The `residplot()` function generates a histogram of the studentized residuals and superimposes a normal curve, kernel density curve, and rug plot. It doesn???t require the car package.
  
```{r residplot_function}
residplot <- function(fit, nbreaks=10){
    z <- rstudent(fit)
    hist(z, breaks = nbreaks, freq = FALSE, 
         xlab = "Studentized Residual",
         main = "Distribution of Errors")
    rug(jitter(z), col="brown")
    curve(dnorm(x, mean = mean(z), sd=sd(z)),
          add=TRUE, col="blue", lwd=2)
    lines(density(z)$x, density(z)$y,
          col="red", lwd=2, lty=2)
    legend("topright", 
           legend = c("Normal Curve", "Kernel Density Curve"),
           lty=1:2, col=c("blue","red"), cex=0.7)
}
residplot(fit)
```

  - INDEPENDENCE OF ERRORS
    - the best way to assess whether the dependent variable values (and thus the residuals) are independent is from your knowledge of how the data were collected. For example, time series data will often display autocorrelation???observations collected closer in time will be more correlated with each other than with observations distant in time. 
    - **Durbin???Watson** test to the multiple regression problem with the following code
```{r Durbin???Watson}
durbinWatsonTest(fit)
# The nonsignificant p-value (p=0.282) suggests a lack of autocorrelation, and conversely an independence of errors. The lag value (1 in this case) indicates that each observa- tion is being compared with the one next to it in the dataset.
```

  - LINEARITY: You can look for evidence of nonlinearity in the relationship between the dependent variable and the independent variables by using component plus residual plots (also known as partial residual plots). The plot is produced by `crPlots()` function in the car package.
```{r linearity_plot}
library(car)
crPlots(fit)
# Nonlinearity in any of these plots sug- gests that you may not have adequately modeled the functional form of that predictor in the regression. 
```
  
  - HOMOSCEDASTICITY: The `car` package also provides two useful functions for identifying **non-constant error variance**. The `ncvTest()` function produces a score test of the hypothesis of constant error variance against the alternative that the error variance changes with the level of the fitted values. A significant result suggests heteroscedasticity (nonconstant error variance). The `spreadLevelPlot()` function creates a scatter plot of the absolute standardized residuals versus the fitted values, and superimposes a line of best fit.    
```{r homoscedasticity}
library(car)
ncvTest(fit) #The score test is nonsignificant (p = 0.19), suggesting that you???ve met the constant variance assumption.
spreadLevelPlot(fit)
#The points form a random horizontal band around a horizontal line of best fit. If you???d vio- lated the assumption, you???d expect to see a nonhorizontal line. The suggested power transformation in listing 8.7 is the suggested power p (Yp) that would stabilize the nonconstant error variance.
```
  
### Global validation of linear model assumption
- the `gvlma()` function in the `gvlma` package. Written by Pena and Slate (2006), the `gvlma()` function performs a global validation of linear model assumptions as well as separate evaluations of **skewness, kurtosis, and heteroscedasticity**.
    
```{r Globa_test_of_linear_model_assumptions}
library(gvlma)
gvmodel <- gvlma(fit)
summary(gvmodel)
# If the decision line had indicated that the assumptions were violated (say, p < 0.05), you???d have had to explore the data using the previous methods discussed in this section to determine which as- sumptions were the culprit.
```
    
### Multicollinearity
- Multicollinearity can be detected using a statistic called the **variance inflation factor (VIF)**. For any predictor variable, the square root of the VIF indicates the degree to which the confidence interval for that variable???s regression parameter is expanded relative to a model with uncorrelated predictors (hence the name). VIF values are provided by the vif() function in the car package. 
- As a general rule, sqrt(vif) > 2 indicates a multicollinearity problem. The code is provided in the following listing. The results indicate that multicollinearity isn???t a problem with our predictor variables.
```{r Evaluating_multicollinearity}
library(car)
vif(fit)
sqrt(vif(fit)) > 2
```
    
## Unusual observations
### Outliers
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
