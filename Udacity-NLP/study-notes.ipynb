{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Natural Language Processing\n",
    "## Building an NLP Pipeline\n",
    "- Text processing\n",
    "- Feature extraction\n",
    "- Modelling\n",
    "\n",
    "### Text processing\n",
    "- format from: html, pdf, docs, voice, books etc\n",
    "- format to: plain text (stem, punctuation, stop words etc)\n",
    "\n",
    "### Feature extraction\n",
    "- depends on what kind of model you're using and what task you're tying to accomplish.\n",
    "  - if you want to use a graph based model to extract insights, you may want to represent your words as symbolic nodes with relationships between them like **WordNet**.\n",
    "  - for statistical models, you need some sort of numerical representation.\n",
    "- think about the end goal\n",
    "  - if you're tying to perform a document level task, such as spam detection or sentiment analysis, you may want to use a per document representations such as **bag-of-words** or **doc2vec**.\n",
    "  - if you want to work with individual words and phrases, such as for text generation or machine translation, you'll need a word level representation such as **word2vec** or **glove**\n",
    "\n",
    "### Modelling\n",
    "- designing a model: statistical or machine learning model\n",
    "- fitting its parameters to training data using an optimization procedure\n",
    "- using it to make predictions about unseen data\n",
    "\n",
    "### Part-of-Speech Tagging\n",
    "- Note: **Part-of-speech** tagging using a predefined grammar but limited solution. It can be very tedious and error-prone for a large corpus of text, since you have to account for all possible sentence structures and tags!\n",
    "- There are other more advanced forms of POS tagging that can learn sentence structures and tags from given data, including **Hidden Markov Models (HMMs)** and **Recurrent Neural Networks (RNNs)***.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/qingqing/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/qingqing/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('I', 'PRP'),\n",
       " ('always', 'RB'),\n",
       " ('lie', 'VBP'),\n",
       " ('down', 'RP'),\n",
       " ('to', 'TO'),\n",
       " ('tell', 'VB'),\n",
       " ('a', 'DT'),\n",
       " ('lie', 'NN')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk \n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk import pos_tag, word_tokenize\n",
    "# Tag parts of speech (PoS)\n",
    "sentence = word_tokenize(\"I always lie down to tell a lie\")\n",
    "pos_tag(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('maxent_ne_chunker')\n",
    "from nltk import pos_tag, ne_chunk\n",
    "from nltk.tokenize import word_tokenize \n",
    "# Recognize named entities in a tagged sentence \n",
    "ne_chunk(pos_tag(word_tokenize(\"Antonio joined Udacity Inc. in California.\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words \n",
    "- To obtain a bag of words from a piece of raw text, need to simply apply appropriate text processing steps: \n",
    "    - cleanning \n",
    "    - normalizing\n",
    "    - splitting into words \n",
    "    - stemming\n",
    "    - lemmatization \n",
    "- treat the resulting tokens as an un-ordered collection of set \n",
    "_But keeping these as separeate sets is very inefficient, they may have differnt sizes, may contain different words and are hard to compare, and words may appear multiple times_\n",
    "\n",
    "- a more useful approach is to turn each document into a vector of numbers represing how many times each word occurs in a document \n",
    "- A set of document is called corpus and this gives the context of the vectors to be calculated \n",
    "    - first: collect all the unique words present in your corpus to form your vocabulary \n",
    "    - second: arrange these words in some order and let them form the vector element positions or columns of a table and assume each document is a row \n",
    "    - third: count the number of occurrences of each word in eacch document and enter the value in the respective column, at this stage, it is easier to think of this as a **Document-Term-Matrix**, illustratin the relationship between documents in rows and words or terms in columns \n",
    "- One possibility is to compare documents based on how many words they have in common or how similar their term frequencies are \n",
    "    - **dot product between the two row vectores**: is the sum of the products of corresponding elements. greater the products, more similar the two vectors are. but  it only capture the overlap not affacted by other values that are not uncommon\n",
    "    - **cosine similarity**: divide the dot product of two vectors by the product of their magnitudes or Euclidean norms "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "- tfidf(t,d,D) = tf(t,d) * idf(t,D) = term frequency * inverse document frequency = count(t,d)/|d| * log(|D|/|{d belongs to D: t belongs to d}|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot Encoding \n",
    "- treat each word like a class \n",
    "- assign it a vector that has one in a single pre-determined position for that word and zero everywhere else, just like the bag-of words idea, only that we keeyp a single word in each bag and build a vector for it "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings \n",
    "- control word representation by limiting it to a fixed-size vector \n",
    "    - if two words are similar in meaning, they should be closer to each other compared to words that are not\n",
    "    - if tow words have a similar difference in their meanings, they should be approximately equally separated in the embedded space\n",
    "    - can use such a representation for a variety of purposes like finding synonyms and analogies, identifying concepts around which words are clustered, classifying words as positive, negative, neutral,etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-SNE (t-Distributed Stochastic Neighbor Embedding)\n",
    "- a dimensionality reduction technique that can map high dimensional vectors to a lower dimensional space. \n",
    "- when applying transformation, it tries to maintain relative distances between objects, so that similar ones stay closer together while dissimilar objects stay further apart. good visualization for word-embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voice user interfaces \n",
    "- Changes \n",
    "    - Variability \n",
    "        - Pitch \n",
    "        - Volume \n",
    "        - Speed\n",
    "    - Ambiguity \n",
    "        - Word Boundaries \n",
    "        - Spelling \n",
    "        - Context "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language models \n",
    "- Deep Neural Networks(**DNN**) as Speech Models\n",
    "- speech --> features --> acoustic model --> phonemes --> words --> language model --> text\n",
    "    - speech --> features: \n",
    "        - MFCC: To extract relevant patterns \n",
    "        - CNN: Also finds relevant patterns \n",
    "    - features --> words:\n",
    "        - HMM's: Time series data & Sequencing \n",
    "        - RNN: Also time series \n",
    "        - CTC: Sequencing \n",
    "    - words --> text \n",
    "        - N-grams: Could still be used \n",
    "        - NLM: Netural Language model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
